[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "search_prompt",
        "importPath": "search",
        "description": "search",
        "isExtraImport": true,
        "detail": "search",
        "documentation": {}
    },
    {
        "label": "create_stuff_documents_chain",
        "importPath": "langchain.chains.combine_documents",
        "description": "langchain.chains.combine_documents",
        "isExtraImport": true,
        "detail": "langchain.chains.combine_documents",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "InMemoryCache",
        "importPath": "langchain_core.caches",
        "description": "langchain_core.caches",
        "isExtraImport": true,
        "detail": "langchain_core.caches",
        "documentation": {}
    },
    {
        "label": "set_llm_cache",
        "importPath": "langchain.globals",
        "description": "langchain.globals",
        "isExtraImport": true,
        "detail": "langchain.globals",
        "documentation": {}
    },
    {
        "label": "PGVector",
        "importPath": "langchain_postgres",
        "description": "langchain_postgres",
        "isExtraImport": true,
        "detail": "langchain_postgres",
        "documentation": {}
    },
    {
        "label": "PGVector",
        "importPath": "langchain_postgres",
        "description": "langchain_postgres",
        "isExtraImport": true,
        "detail": "langchain_postgres",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "embedding_generator",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def embedding_generator() -> OllamaEmbeddings:\n    return OllamaEmbeddings(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\"),\n    )\ndef model() -> ChatOllama:\n    return ChatOllama(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"OLLAMA_MODEL\", \"llama3.2:1b\"),\n    )",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def model() -> ChatOllama:\n    return ChatOllama(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"OLLAMA_MODEL\", \"llama3.2:1b\"),\n    )\ndef store() -> PGVector:\n    return PGVector(\n        embeddings=embedding_generator(),\n        collection_name=collection_name(),\n        connection=connection_url(),",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def store() -> PGVector:\n    return PGVector(\n        embeddings=embedding_generator(),\n        collection_name=collection_name(),\n        connection=connection_url(),\n        use_jsonb=True,\n    )\ndef collection_name() -> str:\n    return os.getenv(\"PG_VECTOR_COLLECTION_NAME\", \"\")\ndef connection_url() -> str:",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "collection_name",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def collection_name() -> str:\n    return os.getenv(\"PG_VECTOR_COLLECTION_NAME\", \"\")\ndef connection_url() -> str:\n    return os.getenv(\"DATABASE_URL\", \"\")\ndef main():\n    question = input(\"Realize sua pergunta:\\n\")\n    # Busca top-k documentos (pega só os Documents)\n    search_results = store().similarity_search_with_score(question, k=10)\n    documents = [d[0] for d in search_results]\n    # Cria o PromptTemplate esperado pela chain (context + pergunta)",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "connection_url",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def connection_url() -> str:\n    return os.getenv(\"DATABASE_URL\", \"\")\ndef main():\n    question = input(\"Realize sua pergunta:\\n\")\n    # Busca top-k documentos (pega só os Documents)\n    search_results = store().similarity_search_with_score(question, k=10)\n    documents = [d[0] for d in search_results]\n    # Cria o PromptTemplate esperado pela chain (context + pergunta)\n    chain = search_prompt(question)\n    if not chain:",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.chat",
        "description": "src.chat",
        "peekOfCode": "def main():\n    question = input(\"Realize sua pergunta:\\n\")\n    # Busca top-k documentos (pega só os Documents)\n    search_results = store().similarity_search_with_score(question, k=10)\n    documents = [d[0] for d in search_results]\n    # Cria o PromptTemplate esperado pela chain (context + pergunta)\n    chain = search_prompt(question)\n    if not chain:\n        print(\"Não foi possível iniciar o chat. Verifique os erros de inicialização.\")\n        return",
        "detail": "src.chat",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def file_path() -> str:\n    return os.getenv(\"PDF_PATH\", \"\")\ndef splitter() -> RecursiveCharacterTextSplitter:\n    return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndef embedding() -> OllamaEmbeddings:\n    return OllamaEmbeddings(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\"),\n    )\ndef collection_name() -> str:",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "splitter",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def splitter() -> RecursiveCharacterTextSplitter:\n    return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndef embedding() -> OllamaEmbeddings:\n    return OllamaEmbeddings(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\"),\n    )\ndef collection_name() -> str:\n    return os.getenv(\"PG_VECTOR_COLLECTION_NAME\", \"\")\ndef connection_url() -> str:",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "embedding",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def embedding() -> OllamaEmbeddings:\n    return OllamaEmbeddings(\n        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\"),\n        model=os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\"),\n    )\ndef collection_name() -> str:\n    return os.getenv(\"PG_VECTOR_COLLECTION_NAME\", \"\")\ndef connection_url() -> str:\n    return os.getenv(\"DATABASE_URL\", \"\")\ndef create_store() -> PGVector:",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "collection_name",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def collection_name() -> str:\n    return os.getenv(\"PG_VECTOR_COLLECTION_NAME\", \"\")\ndef connection_url() -> str:\n    return os.getenv(\"DATABASE_URL\", \"\")\ndef create_store() -> PGVector:\n    return PGVector(\n        embeddings=embedding(),\n        collection_name=collection_name(),\n        connection=connection_url(),\n        use_jsonb=True,",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "connection_url",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def connection_url() -> str:\n    return os.getenv(\"DATABASE_URL\", \"\")\ndef create_store() -> PGVector:\n    return PGVector(\n        embeddings=embedding(),\n        collection_name=collection_name(),\n        connection=connection_url(),\n        use_jsonb=True,\n    )\ndef main():",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "create_store",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def create_store() -> PGVector:\n    return PGVector(\n        embeddings=embedding(),\n        collection_name=collection_name(),\n        connection=connection_url(),\n        use_jsonb=True,\n    )\ndef main():\n    # Carrega o PDF para ler seu conteúdo\n    pdfLoader = PyPDFLoader(file_path())",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.ingest",
        "description": "src.ingest",
        "peekOfCode": "def main():\n    # Carrega o PDF para ler seu conteúdo\n    pdfLoader = PyPDFLoader(file_path())\n    documents = pdfLoader.load()\n    # Utiliza o recursive text splitter para quebrar o documento em chunks\n    chunks = splitter().split_documents(documents)\n    # Aplica uma limpeza de metadados vazios ou nulos\n    enrich_document = [\n        Document(\n            page_content=document.page_content,",
        "detail": "src.ingest",
        "documentation": {}
    },
    {
        "label": "search_prompt",
        "kind": 2,
        "importPath": "src.search",
        "description": "src.search",
        "peekOfCode": "def search_prompt(question) -> PromptTemplate | None:\n    if not question:\n        return\n    message = PromptTemplate(\n        input_variables=[\"context\", \"pergunta\"], template=PROMPT_TEMPLATE\n    )\n    return message",
        "detail": "src.search",
        "documentation": {}
    },
    {
        "label": "PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "src.search",
        "description": "src.search",
        "peekOfCode": "PROMPT_TEMPLATE = \"\"\"\nCONTEXTO:\n{context}\nREGRAS:\n- Responda somente com base no CONTEXTO.\n- Se a informação não estiver explicitamente no CONTEXTO, responda:\n  \"Não tenho informações necessárias para responder sua pergunta.\"\n- Nunca invente ou use conhecimento externo.\n- Nunca produza opiniões ou interpretações além do que está escrito.\nEXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:",
        "detail": "src.search",
        "documentation": {}
    }
]